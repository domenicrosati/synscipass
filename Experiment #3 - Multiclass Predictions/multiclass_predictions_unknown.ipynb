{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-18T16:50:56.018299Z",
     "iopub.status.busy": "2022-07-18T16:50:56.017959Z",
     "iopub.status.idle": "2022-07-18T16:50:56.024884Z",
     "shell.execute_reply": "2022-07-18T16:50:56.023741Z",
     "shell.execute_reply.started": "2022-07-18T16:50:56.018269Z"
    },
    "papermill": {
     "duration": 1.50894,
     "end_time": "2022-06-09T12:47:53.268738",
     "exception": false,
     "start_time": "2022-06-09T12:47:51.759798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T16:48:31.295523Z",
     "iopub.status.busy": "2022-07-18T16:48:31.295142Z",
     "iopub.status.idle": "2022-07-18T16:48:31.301058Z",
     "shell.execute_reply": "2022-07-18T16:48:31.299937Z",
     "shell.execute_reply.started": "2022-07-18T16:48:31.295492Z"
    },
    "papermill": {
     "duration": 0.014006,
     "end_time": "2022-06-09T12:47:53.300986",
     "exception": false,
     "start_time": "2022-06-09T12:47:53.28698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = 0 if torch.cuda.is_available() else -1\n",
    "INPUT_COL_NAME = 'text'\n",
    "TARGET_COL_NAME = 'fake'\n",
    "SMOKE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_labels = {\n",
    "    \"generate\": 1,\n",
    "    \"paraphrase\": 1,\n",
    "    \"real\": 0,\n",
    "    \"translate\": 1,\n",
    "    \"unknown\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T16:48:32.168727Z",
     "iopub.status.busy": "2022-07-18T16:48:32.167604Z",
     "iopub.status.idle": "2022-07-18T16:48:32.199633Z",
     "shell.execute_reply": "2022-07-18T16:48:32.198752Z",
     "shell.execute_reply.started": "2022-07-18T16:48:32.168665Z"
    },
    "papermill": {
     "duration": 0.639346,
     "end_time": "2022-06-09T12:47:53.958602",
     "exception": false,
     "start_time": "2022-06-09T12:47:53.319256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unknown_df = pd.read_csv(\"../detecting-generated-scientific-papers/fake_papers_train_part_public.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_df['type'] = unknown_df['fake'].apply(lambda x: 'unknown' if 1 else 'real' )\n",
    "unknown_df['model'] = unknown_df['fake'].apply(lambda x: 'unknown' if 1 else 'real' )\n",
    "unknown_df['tool'] = unknown_df['fake'].apply(lambda x: 'unknown' if 1 else 'real' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = unknown_df\n",
    "test_df['pred'] = test_df['tool'].apply(lambda x: tool_labels[x])\n",
    "if SMOKE:\n",
    "    test_df = test_df[:SMOKE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    { \n",
    "        \"name\": \"multiclass\",\n",
    "        \"model\": \"anon/deberta-v3-large-finetuned-synthetic-multi-class\"\n",
    "    },\n",
    "]\n",
    "datasets = ['all', 'generate', 'paraphrase', 'translate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T16:48:35.892107Z",
     "iopub.status.busy": "2022-07-18T16:48:35.891384Z",
     "iopub.status.idle": "2022-07-18T16:49:36.610531Z",
     "shell.execute_reply": "2022-07-18T16:49:36.609558Z",
     "shell.execute_reply.started": "2022-07-18T16:48:35.892072Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, texts):\n",
    "    preds = []\n",
    "    ds = Dataset.from_dict({\n",
    "        \"texts\": list(texts.fillna(''))\n",
    "    })\n",
    "    for pred in tqdm(model(KeyDataset(ds, \"texts\"), batch_size=1), total=len(ds)):\n",
    "        preds.append(\n",
    "            pred\n",
    "        )\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def coverage_risk(confidences: np.ndarray, accuracies: np.ndarray):\n",
    "    # From https://github.com/sleep3r/garrus/blob/a6fd1d44b06285918cefe54f421a004dc6f315cb/garrus/metrics/aurc.py\n",
    "    sort_values = sorted(zip(confidences, accuracies), key=lambda x: x[0], reverse=True)\n",
    "    sort_conf, sort_acc = zip(*sort_values)\n",
    "    risk_list = []\n",
    "    coverage_list = []\n",
    "    risk = 0\n",
    "    for i in range(len(sort_conf)):\n",
    "        coverage = (i + 1) / len(sort_conf)\n",
    "        coverage_list.append(coverage)\n",
    "\n",
    "        if sort_acc[i] == 0:\n",
    "            risk += 1\n",
    "\n",
    "        risk_list.append(risk / (i + 1))\n",
    "    return risk_list, coverage_list\n",
    "\n",
    "\n",
    "def compute_area_under_risk_coverage_age(confidences: np.ndarray, accuracies: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Area Under Risk-Coverage.\n",
    "    $$ AURC (\\kappa, f \\mid V_{n}) = \\frac{1}{n} \\sum_{\\theta \\in \\Theta} \\hat{r} (f, g_{\\theta} \\mid V_{n}) $$\n",
    "    From https://github.com/sleep3r/garrus/blob/a6fd1d44b06285918cefe54f421a004dc6f315cb/garrus/metrics/aurc.py\n",
    "    \"\"\"\n",
    "    risk_list, coverage_list = coverage_risk(confidences, accuracies)\n",
    "    risk_coverage_curve_area = 0\n",
    "    for risk_value in risk_list:\n",
    "        risk_coverage_curve_area += risk_value * (1 / len(risk_list))\n",
    "\n",
    "    aurc = risk_coverage_curve_area\n",
    "    return float(aurc), risk_list, coverage_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    'LABEL_0': 1,\n",
    "    'LABEL_1': 1,\n",
    "    'LABEL_2': 0,\n",
    "    'LABEL_3': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting results for multiclass\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe19cd07b14e43cd993d17e12c9aed6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79157f8348a64d1dba00059d8c646bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8702400755fa43209b21e2630de36b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207902deec4140138c63cc31708de801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for model in models:\n",
    "    print(f\"Getting results for {model['name']}\")\n",
    "    mdl = pipeline(\"text-classification\", model['model'], device=DEVICE)\n",
    "    for dataset in datasets:\n",
    "        if dataset == 'all':\n",
    "            df = test_df\n",
    "        else:\n",
    "            df = test_df.loc[\n",
    "                (test_df['tool'] == dataset) | (test_df['tool'] == 'real')\n",
    "            ]\n",
    "        test_predictions = get_predictions(mdl, test_df[INPUT_COL_NAME])\n",
    "\n",
    "        confidences = [\n",
    "            pred['score']\n",
    "            for pred in test_predictions\n",
    "        ]\n",
    "        predictions =  [\n",
    "            label_map[pred['label']]\n",
    "            for pred in test_predictions\n",
    "        ]\n",
    "        accuracies = [\n",
    "            0 if prediction != y_true else 1 for (prediction, y_true)\n",
    "            in zip(predictions, test_df[\"pred\"])\n",
    "        ]\n",
    "        aurc, risk_list, coverage_list = compute_area_under_risk_coverage_age(confidences, accuracies)\n",
    "        f1 = f1_score(test_df[\"pred\"], predictions)\n",
    "        precision = precision_score(test_df[\"pred\"], predictions)\n",
    "        recall = recall_score(test_df[\"pred\"], predictions)\n",
    "\n",
    "        acc = accuracy_score(test_df[\"pred\"], predictions)\n",
    "        data = {\n",
    "            \"model\": model['model'],\n",
    "            \"name\": model['name'],\n",
    "            \"aurc\": aurc,\n",
    "            \"f1\": f1,\n",
    "           \n",
    "            \"precision\": precision,\n",
    "          \n",
    "            \"recall\": recall,\n",
    "     \n",
    "            \"acc\": acc,\n",
    "            \"dataset\": dataset,\n",
    "            \"risk_list\": risk_list,\n",
    "            \"coverage_list\": coverage_list,\n",
    "            \"label_distribution\": list(zip([\n",
    "                pred['label'] for pred in test_predictions\n",
    "            ], accuracies))\n",
    "        }\n",
    "        results.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-18T16:49:53.230309Z",
     "iopub.status.busy": "2022-07-18T16:49:53.229396Z",
     "iopub.status.idle": "2022-07-18T16:49:53.243703Z",
     "shell.execute_reply": "2022-07-18T16:49:53.242500Z",
     "shell.execute_reply.started": "2022-07-18T16:49:53.230268Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    results\n",
    ").to_csv('./multi_class_unknown_evaluation_results_smoke.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f3bfb1298c86f91006802b95f6b40296fd6ef213ba2b003e8e487796664083"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
